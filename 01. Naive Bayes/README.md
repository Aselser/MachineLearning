## Na√Øve Bayes Classifier
## üìò Theory: Na√Øve Bayes Classifier

The **Na√Øve Bayes classifier** is a probabilistic model based on **Bayes‚Äô Theorem**, with the ‚Äúna√Øve‚Äù assumption that all features are conditionally independent given the class.

### Bayes‚Äô Theorem
For a class C and features (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô):

P(C | x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô) = [ P(C) √ó Œ† P(x·µ¢ | C) ] / P(x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô)

- **P(C):** prior probability of class C  
- **P(x·µ¢ | C):** likelihood of feature x·µ¢ given class C  
- **P(C | x‚ÇÅ, ‚Ä¶, x‚Çô):** posterior probability of class C given the features  

---

### Classification Rule
We assign the class with the highest posterior probability:

ƒà = argmax‚ÇçC‚Çé [ P(C) √ó Œ† P(x·µ¢ | C) ]

Since the denominator P(x‚ÇÅ, ‚Ä¶, x‚Çô) is constant for all classes, it can be ignored in practice.

---

### Laplace Smoothing
If a feature value never appears in training data for a given class, its probability would be zero.  
To avoid this, we use **Laplace smoothing**:

P(x·µ¢ | C) = ( count(x·µ¢, C) + 1 ) / ( count(C) + k )

- **count(x·µ¢, C):** how many times feature value x·µ¢ appears within class C  
- **count(C):** number of samples in class C  
- **k:** number of possible values for the feature  
